# -*- coding: utf-8 -*-
"""seq2seq_siamese.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ge0znrxoOb0ej4jFmX3zAAVZfLMhvq9S
"""

# !wget https://www.dropbox.com/s/kxbmga5p8j8amjt/cvutProfiles_gnumbers.zip?dl=0
# !mv cvutProfiles_gnumbers.zip?dl=0 cvutProfiles_gnumbers.zip
# !unzip cvutProfiles_gnumbers.zip
# !rm cvutProfiles_gnumbers.zip
#
# !pip install unidecode

from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
import trainer.lstm_hierarchical.model as model
from trainer.modelCheckpoint import ModelCheckpointMLEngine
from keras.optimizers import RMSprop, SGD, Adam
from keras import backend as K


import numpy as np
import argparse
import sdep

CHECKPOINT_FILE_PATH = 'best_model.h5'
#
# import os
# os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"   # see issue #152
# os.environ["CUDA_VISIBLE_DEVICES"] = ""


def contrastive_loss(y_true, y_pred):
    '''Contrastive loss from Hadsell-et-al.'06
    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf
    '''
    margin = 1
    return K.mean(y_true * K.square(y_pred) + (1 - y_true) * K.square(K.maximum(margin - y_pred, 0)))


def main(data_file, job_dir):
    ev = sdep.AuthorityEvaluator(username='andrej', neighbors=100, radius=20, train_size=0.5)

    joint_model = model.create_model((11, 64))

    joint_model.compile(loss=contrastive_loss, optimizer=Adam(lr=0.001))

    # joint_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00006))
    joint_model.summary(line_length=120)

    _, valid_profile = ev.get_train_dataset()

    left, right, label, _ = next(sdep.pairs_generator(valid_profile, model.BATCH_SIZE))
    left = np.array(list(map(lambda x: model.preprocess_quantiles(x.quantiles, model.MAX_TEXT_SEQUENCE_LEN), left)))
    right = np.array(list(map(lambda x: model.preprocess_quantiles(x.quantiles, model.MAX_TEXT_SEQUENCE_LEN), right)))

    joint_model.fit_generator(model.generate_random_fit(ev),
                              steps_per_epoch=model.TRAINING_SAMPLES // model.BATCH_SIZE,
                              epochs=model.EPOCHS,
                              validation_data=([left, right], label),
                              # class_weight={1: 10., -1: 1.},
                              callbacks=[
                                  ModelCheckpointMLEngine(job_dir + '/model.h5', monitor='val_loss', verbose=1,
                                                          save_best_only=True, mode='min'),
                                  EarlyStopping(monitor='val_loss', patience=10, verbose=1),
                                  TensorBoard(log_dir=job_dir + '/log', write_graph=True, embeddings_freq=0)
                                ])


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('--data-file',
                        required=True,
                        type=str,
                        help='Data file local or GCS', nargs='+')
    parser.add_argument('--job-dir',
                        required=True,
                        type=str,
                        help='GCS or local dir to write checkpoints and export model')
    parse_args, _ = parser.parse_known_args()

    main(**parse_args.__dict__)
